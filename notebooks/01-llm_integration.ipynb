{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44927fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqdrant_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FreenMemory\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      5\u001b[39m memory_store = FreenMemory()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from src.db.qdrant_memory import FreenMemory\n",
    "import subprocess\n",
    "\n",
    "memory_store = FreenMemory()\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_ollama_embedding(text, model=\"nomic-embed-text\"):\n",
    "    \"\"\"\n",
    "    Get embedding from Ollama for the given text and model.\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": text\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data.get(\"embedding\")\n",
    "\n",
    "# Define the Ollama Llama3.1 model name for use elsewhere\n",
    "OLLAMA_LLAMA3_MODEL = \"llama3.1\"\n",
    "\n",
    "def ask_freen(prompt, model=\"llama3.1\"):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    return result.stdout.decode().strip()\n",
    "\n",
    "def ask_freen_with_memory(user_input):\n",
    "    # Get embedding for user input using Ollama\n",
    "    user_embedding = get_ollama_embedding(user_input)\n",
    "    memories = memory_store.get_relevant_memories(user_input, top_k=3)\n",
    "    if memories:\n",
    "        memory_block = \"\\n\".join([f\"- {m['payload'].get('text','')}\" for m in memories])\n",
    "        memory_section = f\"Relevant memories:\\n{memory_block}\\n\\n\"\n",
    "    else:\n",
    "        memory_section = \"\"\n",
    "\n",
    "    # Emotional intelligence instructions (no hard-coded AI identity)\n",
    "    base_prompt = (\n",
    "        \"Respond as a friendly, warm, emotionally intelligent, and kind companion. \"\n",
    "        \"Be attentive to the user's feelings, respond with empathy, and use past memories naturally. \"\n",
    "        \"Acknowledge the user's emotions, offer support, and reply in a gentle, understanding, and positive manner.\"\n",
    "    )\n",
    "\n",
    "    def detect_emotion(text):\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in [\"sad\", \"upset\", \"unhappy\", \"depressed\", \"down\"]):\n",
    "            return \"The user may be feeling sad. Please respond with extra empathy and encouragement.\"\n",
    "        elif any(word in text_lower for word in [\"happy\", \"excited\", \"joy\", \"glad\", \"delighted\"]):\n",
    "            return \"The user may be feeling happy. Celebrate their joy and share in their excitement.\"\n",
    "        elif any(word in text_lower for word in [\"angry\", \"mad\", \"frustrated\", \"annoyed\"]):\n",
    "            return \"The user may be feeling upset or angry. Respond calmly and help them feel heard.\"\n",
    "        elif any(word in text_lower for word in [\"worried\", \"anxious\", \"nervous\", \"scared\"]):\n",
    "            return \"The user may be feeling anxious. Offer reassurance and comfort.\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    emotion_instruction = detect_emotion(user_input)\n",
    "    if emotion_instruction:\n",
    "        base_prompt += f\"\\n\\n{emotion_instruction}\"\n",
    "\n",
    "    full_prompt = f\"{base_prompt}\\n\\n{memory_section}User: {user_input}\\nCompanion:\"\n",
    "\n",
    "    response = ask_freen(full_prompt)\n",
    "    # Store both the user input and the response, with embeddings from Ollama\n",
    "    if user_embedding is not None:\n",
    "        memory_store.add_memory({\"text\": f\"User said: {user_input}\", \"embedding\": user_embedding})\n",
    "    else:\n",
    "        memory_store.add_memory(f\"User said: {user_input}\")\n",
    "\n",
    "    response_embedding = get_ollama_embedding(response)\n",
    "    if response_embedding is not None:\n",
    "        memory_store.add_memory({\"text\": f\"Companion replied: {response}\", \"embedding\": response_embedding})\n",
    "    else:\n",
    "        memory_store.add_memory(f\"Companion replied: {response}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "user_question = input(\"Ask a question: \")\n",
    "print(ask_freen_with_memory(user_question))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
