{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm not Freen, but I'm happy to chat with you! I'm a computer program designed to assist and communicate with users in a friendly manner. I don't have feelings or emotions like humans do, so I don't have good or bad days. How can I help you today?\n",
            "Yes, I am a Large Language Model (LLM). My primary function is to understand and respond to human input in a coherent and contextually relevant manner. I'm a type of artificial intelligence designed to process and generate human-like text based on the input provided to me.\n",
            "\n",
            "I have been trained on a massive dataset of text from various sources, which allows me to learn patterns, relationships, and structures within language. This training enables me to:\n",
            "\n",
            "1. **Answer questions**: Provide information on a wide range of topics.\n",
            "2. **Generate text**: Create coherent and contextually relevant responses, articles, or even entire stories.\n",
            "3. **Translate languages**: Translate text from one language to another (though my proficiency may vary depending on the language pair).\n",
            "4. **Summarize content**: Condense long pieces of text into shorter, more digestible summaries.\n",
            "5. **Engage in conversation**: Respond to natural language input, using context and understanding to create a dialogue-like experience.\n",
            "\n",
            "Feel free to ask me anything or provide a topic you'd like to discuss. I'm here to help!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def freen_respond(prompt):\n",
        "    response = requests.post(\n",
        "        \"http://localhost:11434/api/generate\",\n",
        "        json={\"model\": \"llama3.1\", \"prompt\": prompt, \"stream\": False}\n",
        "    )\n",
        "    return response.json()[\"response\"]\n",
        "\n",
        "print(freen_respond(\"Hello Freen, how are you today?\"))\n",
        "print(freen_respond(\"Are u an LLM?\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOdu4vK5mPBLODktz/ckz/O",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
