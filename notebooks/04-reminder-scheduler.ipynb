{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7b87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()  # loads QDRANT_URL and QDRANT_API_KEY from .env\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "COLLECTION_NAME = \"freen_memory\"\n",
    "\n",
    "# Choose sentence-transformers model (small, fast, 384-dim)\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "class FreenMemory:\n",
    "    def __init__(self, qdrant_url=QDRANT_URL, api_key=QDRANT_API_KEY, collection=COLLECTION_NAME):\n",
    "        assert qdrant_url and api_key, \"Set QDRANT_URL and QDRANT_API_KEY in .env\"\n",
    "        self.client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
    "        self.collection = collection\n",
    "        self.embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "        # 384 is the vector dimension for all-MiniLM-L6-v2\n",
    "        self.dim = 384\n",
    "        self._ensure_collection()\n",
    "\n",
    "    def _ensure_collection(self):\n",
    "        # create or recreate collection if needed\n",
    "        try:\n",
    "            # if collection exists, this will raise; safer to check then create if missing\n",
    "            if self.collection not in [c.name for c in self.client.get_collections().collections]:\n",
    "                self.client.create_collection(\n",
    "                    collection_name=self.collection,\n",
    "                    vector_size=self.dim,\n",
    "                    distance=\"Cosine\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            # fallback: recreate collection\n",
    "            # self.client.recreate_collection(collection_name=self.collection, vector_size=self.dim, distance=\"Cosine\")\n",
    "            print(\"Collection check/create resulted in:\", e)\n",
    "\n",
    "    def _embed(self, texts):\n",
    "        # text -> numpy array(s)\n",
    "        emb = self.embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return emb\n",
    "\n",
    "    def add_memory(self, text, meta=None, id=None):\n",
    "        \"\"\"\n",
    "        Add a memory to Qdrant.\n",
    "        - text: short string (the memory)\n",
    "        - meta: dict with optional metadata (e.g., {\"type\":\"chat\", \"timestamp\": 123456789, \"source\":\"voice\"})\n",
    "        - id: optional unique id; if None, we use timestamp-based id\n",
    "        \"\"\"\n",
    "        if meta is None:\n",
    "            meta = {}\n",
    "        if \"timestamp\" not in meta:\n",
    "            meta[\"timestamp\"] = int(time.time())\n",
    "        vector = self._embed([text])[0]\n",
    "        if id is None:\n",
    "            id = f\"{meta['timestamp']}_{abs(hash(text)) % (10**9)}\"\n",
    "        point = {\n",
    "            \"id\": id,\n",
    "            \"vector\": vector.tolist(),\n",
    "            \"payload\": {\"text\": text, **meta}\n",
    "        }\n",
    "        # upsert into qdrant\n",
    "        self.client.upsert(collection_name=self.collection, points=[point])\n",
    "        return id\n",
    "\n",
    "    def get_relevant_memories(self, query_text, top_k=3, min_score=None):\n",
    "        \"\"\"\n",
    "        Return top_k memories for the query_text.\n",
    "        Response: list of dicts: {\"id\",\"score\",\"payload\"}\n",
    "        \"\"\"\n",
    "        q_vec = self._embed([query_text])[0]\n",
    "        hits = self.client.search(collection_name=self.collection, query_vector=q_vec.tolist(), limit=top_k)\n",
    "        # hits are qdrant client SearchResult objects; shape may vary by client version\n",
    "        results = []\n",
    "        for h in hits:\n",
    "            # depending on client version: h.payload, h.score\n",
    "            results.append({\"id\": h.id, \"score\": getattr(h, \"score\", None), \"payload\": getattr(h, \"payload\", {})})\n",
    "        return results\n",
    "\n",
    "    def delete_old_memories(self, older_than_seconds=60*60*24*30):\n",
    "        \"\"\"\n",
    "        Simple pruning: delete memories older than `older_than_seconds`.\n",
    "        This requires that we stored 'timestamp' in payload.\n",
    "        Note: qdrant filter API syntax may vary by client version.\n",
    "        \"\"\"\n",
    "        cutoff = int(time.time()) - older_than_seconds\n",
    "        # Basic approach: search all points with a filter; if client version doesn't support delete by filter, fetch ids and delete in python\n",
    "        try:\n",
    "            # naive fetch â€” get all points (careful with large DB); for prototype this is OK\n",
    "            # Use pagination if many points exist\n",
    "            resp = self.client.scroll(collection_name=self.collection, with_payload=True)\n",
    "            ids_to_delete = []\n",
    "            for item in resp:\n",
    "                ts = item.payload.get(\"timestamp\", 0)\n",
    "                if ts < cutoff:\n",
    "                    ids_to_delete.append(item.id)\n",
    "            if ids_to_delete:\n",
    "                self.client.delete(collection_name=self.collection, points=ids_to_delete)\n",
    "                print(f\"Deleted {len(ids_to_delete)} old memories.\")\n",
    "        except Exception as e:\n",
    "            print(\"Prune failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
